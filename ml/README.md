# MLearning KNN DApp

This example shows how to use an Machine Learning technique integrated with Cartesi Rollups. For this example we will use K-nn with the euclidian distance. Moreover, we solve the classic Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris) problem. The Dapp also can predict an new input give by the user.

## Building the environment

To run the ml DApp example, clone the repository as follows:

```shell
$ git clone https://github.com/cartesi/rollups-examples.git
```

Then, build the back-end for the ml example:

```shell
$ cd rollups-examples/ml
$ make machine
```

## Running the environment

To start the containers in production mode, run:

```shell
$ docker-compose up --build
```

_Note:_ If you decide to use [Docker Compose V2](https://docs.docker.com/compose/cli-command/), make sure you set the [compatibility flag](https://docs.docker.com/compose/cli-command-compatibility/) when executing the command (e.g., `docker-compose --compatibility up`).

_Note:_ If running the make files or docker causes permission failed error, run as superuser.

Allow some time for the infrastructure to be ready.
How much will depend on your system, but after some time showing the error `"concurrent call in session"`, eventually the container logs will repeatedly show the following:

```shell
server_manager_1      | Received GetVersion
server_manager_1      | Received GetStatus
server_manager_1      |   default_rollups_id
server_manager_1      | Received GetSessionStatus for session default_rollups_id
server_manager_1      |   0
server_manager_1      | Received GetEpochStatus for session default_rollups_id epoch 0
```

To stop the containers, first, end the process with `Ctrl + C`.
Then, remove the containers and associated volumes by executing:

```shell
$ docker-compose down -v
```
## Understanding the application

![ml](https://user-images.githubusercontent.com/4421825/155430654-2afcf003-35f7-46a2-9709-af258b33a970.png)



The ml application is a Knn dapp who loads the Iris dataset and uses a KNN to solve the problem. It also receives an input to calculate new predictions. The Iris dataset is a classic problem where The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.

For this base, we have the attribute Information:

1. sepal length in cm
2. sepal width in cm
3. petal length in cm
4. petal width in cm

Following this, the input should bem a string as follows:  
```
{"sl": "2.0", "sw": "3.0", "pl": "4.0", "pw": "3.5"}
```
Where "sl" represents the sepal lenght, "sw" is the sepal width,  "pl" is the petal lenght and the "pw" is the petal width. Note that this JSON string should necessarily be in hexadecimal. In the example above, the hexadecimal should be like below. 
```
0x7b22736c223a2022322e30222c20227377223a2022332e30222c2022706c223a2022342e30222c20227077223a2022332e35227d
```

In the next section, you can see how to interact with the DApp to send inputs and see the notices array.

## Interacting with the application

### Command-Line Input

With the infrastructure in place, go to a separate terminal window and send an input as follows:

```shell
$ docker exec ml_hardhat_1 npx hardhat --network localhost ml:addInput --input "0x7b22736c223a2022322e30222c20227377223a2022332e30222c2022706c223a2022342e30222c20227077223a2022332e35227d"
```

When you receive a response similar to the one below, you know your input was accepted.

```shell
Added input '0x7b22736c223a2022322e30222c20227377223a2022332e30222c2022706c223a2022342e30222c20227077223a2022332e35227d' to epoch '0' (timestamp: 1643998586, signer: 0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266, tx: 0x765a93faa7acc2a15cb2e577c16ffd0c7d106599c62b0b5c9be9dfc7fc9ee03f)
```
### Seeing Results, notices, and more

To verify the notices generated by your inputs, run the command:

```shell
$ curl http://localhost:4000/graphql -H 'Content-Type: application/json' -d '{ "query" : "query getNotice { GetNotice( query: { session_id: \"default_rollups_id\", epoch_index: \"0\", input_index: \"0\" } ) { session_id epoch_index input_index notice_index payload } }" }'
```

The response should be something like this:

```shell
{"data":{"GetNotice":[{"session_id":"default_rollups_id","epoch_index":"0","input_index":"0","notice_index":"0","payload":"63617274657369da"}]}}
```
You can also run those commands in the "playground graphql" host at "http://localhost:4000/graphql", it is easier to understand and manipulate the structures defined in (https://github.com/cartesi/rollups/blob/main/src/reader/src/graphql/typeDefs/typeDefs.graphql). For example, the GetNotice query can be run in the localhost, as shown below.

![image](https://user-images.githubusercontent.com/4421825/152856704-c0c33c13-f695-4d43-bec3-9b6e6cfb9d07.png)


We can see the prediction's payload result in the "payload" attribute. In this example, every input generates only one notice, so you can get the prediction result by checking the notices of an input by the index.

We can also see the full array of notices with the GetEpochStatus query with the command below.

```shell
query{ GetEpochStatus(query:{session_id: "default_rollups_id", epoch_index:"0"})
      { processed_inputs{ input_index 
      result{
        notices{
        payload} } } } }
```

The results will be displayed as shown in the image below.

![image](https://user-images.githubusercontent.com/4421825/152856017-ac301f70-0dd6-42f2-af55-1312ce17ddd8.png)



## Advancing time

To advance time, to simulate the passing of epochs, run:

```shell
$ docker exec ml_hardhat_1 npx hardhat --network localhost util:advanceTime --seconds 864010
```

## Running the environment in host mode

When developing an application, it is often essential to quickly test and debug it. For that matter, it is possible to run the Cartesi Rollups environment in [host mode](../README.md#host-mode) so that the DApp's back-end can be executed directly on the host machine, allowing it to be debugged using standard development tools such as an IDE.

The first step is to run the environment in host mode using the following command:

```shell
docker-compose -f docker-compose-host.yml up --build
```

The next step is to run the ml server in your machine. The application is written in Python, so you need to have `python3` installed.

To start the ml server, run the following commands in a dedicated terminal:

```shell
cd ml/server/
python3 -m venv .env
. .env/bin/activate
pip install -r requirements.txt
HTTP_DISPATCHER_URL="http://127.0.0.1:5004" gunicorn --preload --workers 1 --bind 0.0.0.0:5003 ml:app
```

The ml server will run on port `5003` and send the corresponding notices to port `5004`. After it's successfully started, it should print an output like the following:

```
[2022-01-21 12:38:23,971] INFO in ml: HTTP dispatcher url is http://127.0.0.1:5004
[2022-01-21 12:38:23 -0500] [79032] [INFO] Starting gunicorn 19.9.0
[2022-01-21 12:38:23 -0500] [79032] [INFO] Listening at: http://0.0.0.0:5003 (79032)
[2022-01-21 12:38:23 -0500] [79032] [INFO] Using worker: sync
[2022-01-21 12:38:23 -0500] [79035] [INFO] Booting worker with pid: 79035
```

After that, you can interact with the application typically [as explained above](#interacting-with-the-application).

When you add an input, you should see it being processed by the ml server as follows:

```shell
[2022-02-23 20:45:17,230] INFO in ml: Loading model
[Iris-versicolor] => 0
[Iris-virginica] => 1
[Iris-setosa] => 2
[2022-02-23 20:45:17,247] INFO in ml: The time of execution of evaluation algorithm is :0.016093730926513672
[2022-02-23 20:45:17,247] INFO in ml: Current Scores for Knn: [96.66666666666667, 96.66666666666667, 100.0, 90.0, 100.0]
[2022-02-23 20:45:17,248] INFO in ml: Current Mean Accuracy for Knn in this dataset is : 96.66666666666667
[2022-02-23 20:45:17,248] INFO in ml: Getting input
[2022-02-23 20:45:17,248] INFO in ml: Received advance request body {'metadata': {'msg_sender': '0xf39fd6e51aad88f6f4ce6ab8827279cfffb92266', 'epoch_index': 0, 'input_index': 2, 'block_number': 13, 'time_stamp': 1645659925}, 'payload': '0x7b22736c223a2022322e30222c20227377223a2022332e30222c2022706c223a2022342e30222c20227077223a2022332e35227d'}
[2022-02-23 20:45:17,248] INFO in ml: Printing Body Payload : 0x7b22736c223a2022322e30222c20227377223a2022332e30222c2022706c223a2022342e30222c20227077223a2022332e35227d
[2022-02-23 20:45:17,248] INFO in ml: This should be the sepal lenght 2.0
[2022-02-23 20:45:17,248] INFO in ml: This should be the sepal width 3.0
[2022-02-23 20:45:17,248] INFO in ml: This should be the petal lenght 4.0
[2022-02-23 20:45:17,248] INFO in ml: This should be the petal width 3.5
[2022-02-23 20:45:17,249] INFO in ml: The received input is: [2.0, 3.0, 4.0, 3.5]
[2022-02-23 20:45:17,249] INFO in ml: The time of execution of prediction is :0.00011968612670898438
[2022-02-23 20:45:17,249] INFO in ml: Data={"sl": "2.0", "sw": "3.0", "pl": "4.0", "pw": "3.5"}, Predicted: 0
[2022-02-23 20:45:17,249] INFO in ml: Predicted in Hex: 0x30
[2022-02-23 20:45:17,249] INFO in ml: New PayLoad Added: 0x30
[2022-02-23 20:45:17,250] INFO in ml: Adding notice
[2022-02-23 20:45:17,254] INFO in ml: Received notice status 201 body b'{"index":0}'
[2022-02-23 20:45:17,255] INFO in ml: Finishing
[2022-02-23 20:45:17,259] INFO in ml: Received finish status 202

```

Finally, to stop the containers, removing any associated volumes, execute:

```shell
docker-compose -f docker-compose-host.yml down -v
```

